install.packages("tm")
install.packages("quanteda")
library(tm)
library(quanteda)
library(quanteda)
library(tm)
install.packages("readxl")
getwd()
setwd("C:\Users\Dery Andrian\Downloads\R Analytics\GKR Hemas\News Analytics")
setwd("C:\\Users\\Dery Andrian\\Downloads\\R Analytics\\GKR Hemas\\News Analytics")
data <- read_excel("data\\Data Berita.xlxs")
library(readxl)
data <- read_excel("data\\Data Berita.xlxs")
data <- read_excel("Data Berita.xlxs")
data <- read_excel("data\\Data Berita.xlsx")
data <- read_excel("data\\Data Berita.xlsx")
data\\
data <- read_excel("data\\Data Berita.xlsx")
View(data)
berita <- data$Konten
stopwords <- stopwords("indonesia")  # Menggunakan stopwords bahasa Indonesia
library(tm)
stopwords_getlanguages()
stopwords_getlanguages()
install.packages("tm")
stopwords <- readLines("indonesian_stopwords.txt", encoding = "UTF-8")
install.packages("stopwords") library(stopwords)
install.packages("stopwords")
install.packages("stopwords")
library(stopwords)
stopwords <- stopwords("indonesia")  # Menggunakan stopwords bahasa Indonesia
install.packages("stopwords")
library(stopwords)
install.packages("stopwords")
stopwords <- stopwords("indonesia")  # Menggunakan stopwords bahasa Indonesia
IndonesiaStopWords <- readLines("stoplist.txt")
stopwords <- readLines("data\\stopwords_id_satya.txt")
stopwords <- readLines("stopwords_id_satya.txt")
library(stopwords)
stopwords("indonesia")  # Menggunakan stopwords bahasa Indonesia
stopwords <- stopwords("indonesia")  # Menggunakan stopwords bahasa Indonesia
stopwords_getlanguages()
stopwords <- readLines("stopwords_id_satya.txt", encoding = "UTF-8")
install.packages("textclean")
# Load the stopwords data
stopwords_id <- readLines("stopwords_id.txt")
# Load the textclean package
library(textclean)
# Load the stopwords data
stopwords_id <- readLines("stopwords_id.txt")
# Load the stopwords data
stopwords_id <- readLines("data\\indonesian-stopwords-complete.txt")
library(readr)
install.packages("readr")
library(readr)
# Load the stopwords data
stopwords_id <- readLines("data\\indonesian-stopwords-complete.txt")
getwd()
# Load the stopwords data
stopwords_id <- readLines("data\\indonesian-stopwords-complete.txt")
getwd()
setwd("C:\\Users\\Dery Andrian\\Downloads\\R Analytics\\GKR Hemas\\News Analytics")
# Load the stopwords data
stopwords_id <- read_Lines("data\\indonesian-stopwords-complete.txt")
# Load the stopwords data
stopwords_id <- read_lines("data\\indonesian-stopwords-complete.txt")
corpus <- corpus(berita)
install.packages("tm")
install.packages("readxl")
install.packages("quanteda")
install.packages("textclean")
install.packages("readr")
install.packages("stopwords")
library(tm)
library(quanteda)
library(readxl)
library(stopwords)
library(readr)
install.packages("stopwords")
install.packages("readr")
install.packages("textclean")
library(tm)
library(quanteda)
library(readxl)
library(stopwords)
library(readr)
corpus <- corpus(berita)
corpus_clean <- tm_map(corpus, removeWords, stopwords_id)
corpus_clean <- Corpus(VectorSource(corpus))
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords_id)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords_id)
dtm <- dfm(corpus_clean)
dtm <- dfm(as.character(corpus_clean))
dtm <- dfm(as.character(corpus_clean))
dtm <- dfm(tokens(corpus_clean))
dtm <- dfm(as.character(corpus_clean))
dtm <- dfm(tokens(corpus_clean))
dtm <- dfm(tokens(corpus_clean))
dtm <- dfm(as.character(corpus_clean))
corpus_clean <- as.character(corpus_clean)
dtm <- dfm(tokens(corpus_clean))
dtm <- dfm(corpus_clean)
dtm <- dfm(tokens(corpus_clean))
dtm <- dfm(corpus_clean)
corpus_clean <- as.character(corpus_clean)
dtm <- tokens(corpus_clean)
dtm <- dfm(corpus_clean)
network <- textstat_network(dtm, min_count = 2, min_docfreq = 2)
library(tm)
network <- textstat_network(dtm, min_count = 2, min_docfreq = 2)
install.packages("igraph")
library(igraph)
stopwords_id <- read_lines("data\\indonesian-stopwords-complete.txt")
# Create a corpus of Indonesian text
corpus <- Corpus(VectorSource(data$Konten))
# Remove stopwords from the corpus
corpus <- tm_map(corpus, removeWords, stopwords_id)
# Create a document-term matrix (DTM) from the corpus
dtm <- DocumentTermMatrix(corpus)
# Remove terms that only appear once in the DTM
dtm <- removeSparseTerms(dtm, min_df = 2)
# Remove terms that only appear once in the DTM
dtm <- removeSparseTerms(dtm, min_df = 2)
# Remove terms that only appear once in the DTM
dtm <- removeSparseTerms(dtm)
# Create a network object from the DTM
network <- igraph::graph_from_adjacency_matrix(dtm, mode = "undirected")
# Create a network object from the DTM
network <- igraph::graph_from_adjacency_matrix(as.matrix(dtm), mode = "undirected")
# Create a network object from the DTM
dtm <- t(dtm)
# Plot the network
plot(network)
network <- igraph::graph_from_adjacency_matrix(as.matrix(dtm), mode = "undirected")
# Create a network object from the DTM
dtm <- t(dtm)
network <- igraph::graph_from_adjacency_matrix(as.matrix(dtm), mode = "undirected")
dtm <- t(dtm)
dtm <- rbind(dtm, matrix(0, 10, 5))
# Plot the network
plot(network)
dtm <- rbind(dtm, matrix(0, 10, 5))
network <- igraph::graph_from_adjacency_matrix(as.matrix(dtm), mode = "undirected")
dtm <- t(dtm)
dtm <- rbind(dtm, matrix(0, 10, 10-15))
dtm <- t(dtm)
dtm <- rbind(dtm, matrix(0, 10, 5))
dtm <- t(dtm)
network <- igraph::graph_from_adjacency_matrix(as.matrix(dtm), mode = "undirected")
network <- textstat_network(dtm, min_count = 2, min_docfreq = 2)
library(quanteda)
network <- textstat_network(dtm, min_count = 2, min_docfreq = 2)
plot(network, labelsize = 1, edge_color = "grey", vertex_size = 2, edge_labelsize = 0.8)
library(tm)
network <- textstat_network(dtm, min_count = 2, min_docfreq = 2)
network <- textstat_network(dtm, min_count = 2, min_docfreq = 2)
install.packages("wordcloud")
install.packages("wordcloud2")
installed.packages("RColorBrewer")
installed.packages("SnowballC")
install.packages("ggraph")
library(tm)
library(quanteda)
library(readxl)
library(stopwords)
library(readr)
library(igraph)
library(wordcloud)
library(wordcloud2)
library(SnowballC)
library(RColorBrewer)
library(ggraph)
# Load the textclean package
library(textclean)
getwd()
setwd("C:\\Users\\Dery Andrian\\Downloads\\R Analytics\\GKR Hemas\\News Analytics")
data <- read_excel("data\\Data Berita.xlsx")
berita <- data$Konten
stopwords_id <- read_lines("data\\indonesian-stopwords-complete.txt")
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(data$Konten))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removeWords, stopwords_id)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# keep a copy for stem completion later
myCorpusCopy <- myCorpus
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm
freq.terms <- findFreqTerms(tdm, lowfreq = 20)
freq.terms[1:50]
freq.terms <- findFreqTerms(tdm, lowfreq = 5)
freq.terms[1:50]
#grafik
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 1000)
df <- data.frame(term = names(term.freq), freq = term.freq)
ggplot2::ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
xlab("Terms") + ylab("Count") + coord_flip() +
theme(axis.text=element_text(size=7))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 20,
random.order = F, colors = pal)
d <- data.frame(word = names(word.freq),freq=word.freq)
head(d, n=100)
a <- head(d,n=100)
wordcloud2(a,shape = "cloud",
backgroundColor = "black",
color = 'random-light' ,
size = 0.5)
wordcloud2(demoFreq, figPath = "F:/t.png", size = 1.5,color = "skyblue")
letterCloud(d, word = "anies", wordSize = 1)
letterCloud(d, word = "hemas", wordSize = 1)
letterCloud(d, word = "hemas", wordSize = 1)
ggraph(a, layout = "fr") +
geom_edge_link() +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
(ps_words <- tibble(chapter = seq_along(philosophers_stone),
text = philosophers_stone) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words$word))
(ps_words <- tibble(chapter = seq_along(philosophers_stone),
text = philosophers_stone) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words$word))
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 20,
random.order = F, colors = pal)
wordcloud2(a,shape = "cloud",
backgroundColor = "black",
color = 'random-light' ,
size = 0.5)
ggraph(a, layout = "fr") +
geom_edge_link() +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
ggraph(a, layout = "fr") +
geom_edge_link() +
geom_node_point(color = "lightblue", size = 10) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
ggraph(a, layout = "fr") +
geom_edge_link() +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
freq.terms <- findFreqTerms(tdm, lowfreq = 10)
freq.terms[1:50]
#grafik
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 1000)
df <- data.frame(term = names(term.freq), freq = term.freq)
ggplot2::ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
xlab("Terms") + ylab("Count") + coord_flip() +
theme(axis.text=element_text(size=7))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 20,
random.order = F, colors = pal)
d <- data.frame(word = names(word.freq),freq=word.freq)
head(d, n=100)
a <- head(d,n=100)
wordcloud2(a,shape = "cloud",
backgroundColor = "black",
color = 'random-light' ,
size = 0.5)
?wordcloud2()
wordcloud2(demoFreq, figPath = "F:/t.png", size = 1.5,color = "skyblue")
ggraph(a, layout = "fr") +
geom_edge_link() +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
install.packages("IRkernel")
setwd("C:\\Users\\Dery Andrian\\Downloads\\R Analytics\\GKR Hemas\\News Analytics")
data <- read_excel("data\\Data Gabungan.xlsx")
corpus <- Corpus(VectorSource(data$Konten))
myStopwords <- c(stopwords_id, c("gkr", "hemas","ajak","masuki","unggah","breaking","pastikan","miliki","datangi","penyesuaian","kangen","pastikan"))
corpus <- tm_map(corpus, content_transformer(tolower))   # Mengubah teks menjadi huruf kecil
corpus <- tm_map(corpus, removePunctuation)              # Menghapus tanda baca
corpus <- tm_map(corpus, removeNumbers)                  # Menghapus angka
corpus <- tm_map(corpus_clean, removeWords, myStopwords) # hapus stopwords
data <- read_excel("data\\Data Gabungan.xlsx")
corpus <- Corpus(VectorSource(data$Konten))
myStopwords <- c(stopwords_id, c("gkr", "hemas","ajak","masuki","unggah","breaking","pastikan","miliki","datangi","penyesuaian","kangen","pastikan"))
# Pra-pemrosesan teks
corpus <- tm_map(corpus, content_transformer(tolower))   # Mengubah teks menjadi huruf kecil
corpus <- tm_map(corpus, removePunctuation)              # Menghapus tanda baca
corpus <- tm_map(corpus, removeNumbers)                  # Menghapus angka
corpus <- tm_map(corpus_clean, removeWords, myStopwords) # hapus stopwords
corpus_clean <-
corpus <- tm_map(corpus_clean, removeWords, myStopwords)
myStopwords <- c(stopwords_id)
stopwords_id <- read_lines("data\\indonesian-stopwords-complete.txt")
myStopwords <- c(stopwords_id, c("gkr", "hemas","ajak","masuki","unggah","breaking","pastikan","miliki","datangi","penyesuaian","kangen","pastikan"))
corpus <- tm_map(corpus, content_transformer(tolower))   # Mengubah teks menjadi huruf kecil
corpus <- tm_map(corpus, removePunctuation)              # Menghapus tanda baca
corpus <- tm_map(corpus, removeNumbers)                  # Menghapus angka
corpus <- tm_map(corpus_clean, removeWords, myStopwords)
stopwords_id <- read_lines("data\\indonesian-stopwords-complete.txt")
myStopwords <- c(stopwords_id, c("gkr", "hemas","ajak","masuki","unggah","breaking","pastikan","miliki","datangi","penyesuaian","kangen","pastikan"))
# Pra-pemrosesan teks
corpus <- tm_map(corpus, content_transformer(tolower))   # Mengubah teks menjadi huruf kecil
corpus <- tm_map(corpus, removePunctuation)              # Menghapus tanda baca
corpus <- tm_map(corpus_clean, removeWords, myStopwords)
data <- read_excel("data\\Data Gabungan.xlsx")
stopwords_id <- read_lines("data\\indonesian-stopwords-complete.txt")
corpus <- Corpus(VectorSource(data$Konten))
myStopwords <- c(stopwords_id, c("gkr", "hemas","ajak","masuki","unggah","breaking","pastikan","miliki","datangi","penyesuaian","kangen","pastikan"))
# Preprocessing text
corpus <- tm_map(corpus, content_transformer(tolower))   # Convert text to lowercase
corpus <- tm_map(corpus, removePunctuation)              # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                  # Remove numbers
corpus <- tm_map(corpus, removeWords, myStopwords)       # Remove custom stopwords
corpus <- tm_map(corpus, stripWhitespace)
# Create a document-term matrix
dtm <- DocumentTermMatrix(corpus)
co_matrix <- t(as.matrix(dtm)) %*% as.matrix(dtm)
co_matrix <- (co_matrix + t(co_matrix)) / 2
co_matrix <- as.matrix(co_matrix)
# Check if the matrix is symmetric
if (!isSymmetric(co_matrix)) {
co_matrix <- forceSymmetric(co_matrix)
}
# Convert the co-occurrence matrix to an adjacency matrix
adj_matrix <- ifelse(co_matrix > 0, 1, 0)
# Convert the adjacency matrix to an igraph graph
network <- graph_from_adjacency_matrix(adj_matrix, mode = "undirected", weighted = TRUE)
network
g <- as_tbl_graph(network)
library(readxl)
library(ggplot2)
library(RColorBrewer)
library(scales)
library(extrafont)
library(ggplot2)
library(tidyverse)
library(tidygraph)
library(wordcloud)
library(wordcloud2)
library(webshot)
library(tm)
library(igraph)
library(ggraph)
library(qgraph)
library(networkD3)
library(htmlwidgets)
library(htmltools)
library(IRdisplay)
g <- as_tbl_graph(network)
g
# Filter edges with weight > 3
filtered_edges <- E(g)[E(g)$weight >= 10]
# Create a new graph with filtered edges
filtered_graph <- delete_edges(g, E(g) - filtered_edges)
filtered_graph
# Compute the Fruchterman-Reingold layout
layout <- layout_with_fr(filtered_graph)
# Increase the repulsion by adjusting the layout manually
layout <- layout * 100
node_distance <- 100
# Adjust the distance between nodes in the layout
layout_adjusted <- layout + node_distance
ggraph(filtered_graph, layout = layout_adjusted) +
geom_edge_link(aes(width = weight), alpha = 0.2) +
geom_node_point(size = 1) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void() +
theme(plot.margin = margin(1, 1, 1, 1, "cm")) +
coord_fixed()
# Convert filtered_graph to a format suitable for NetworkD3
nodes <- data.frame(id = V(filtered_graph)$name, group = 1, stringsAsFactors = FALSE)
links <- as.data.frame(get.edgelist(filtered_graph), stringsAsFactors = FALSE)
names(links) <- c("source", "target")
forceNetwork(Links = links, Nodes = nodes, Source = "source", Target = "target",
NodeID = "id", Group = "group", width = 800, height = 600)
# Create NetworkD3 plot
net <- forceNetwork(Links = links, Nodes = nodes, Source = "source", Target = "target",
NodeID = "id", Group = "group", width = 800, height = 600)
# Render the graph using htmlwidgets
net_widget <- htmlwidgets::onRender(net, "document.getElementById('graphDiv')")
# Generate HTML code for the graph
graph_html <- as.character(tags$div(id = "graphDiv", net_widget))
# Display the graph in the notebook
display_html(graph_html)
